---
title: "An introduction to Stan programming"
author: "Sean Anderson"
output:
  html_document:
    toc: true
    toc_float: true
---

```{r, echo=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 6,
  fig.asp = 0.618,
  fig.align = "center"
)
show_file = function(file) {
  cat(paste(readLines(file), collapse = "\n"))
}
```

```{r, warning=FALSE, message=FALSE}
library(tidyverse)
theme_set(theme_light())
```

```{r}
library(rstan)
```

```{r}
rstan_options(auto_write = TRUE)
```

```{r}
# options(mc.cores = parallel::detectCores())
```

Let's simulate some data to fit a very simple Stan model to. We will focus on a linear model with normally distributed errors.

```{r}
set.seed(42)
N <- 30
x <- rnorm(N, 0, 0.5)
alpha <- -0.2
beta <- 0.4
sigma <- 0.3
y <- rnorm(N, mean = alpha + x * beta, sd = sigma)
dat <- tibble(x = x, y = y)
```

```{r}
ggplot(dat, aes(x, y)) + geom_point()
```

```{r}
show_file("stan-models/lm.stan")
```

```{r}

```

The first time we run the next code chunk, Stan will translate our model into C++ and compile it. This will take little bit of time. After that, assuming we set `rstan_options(auto_write = TRUE)`, Stan will avoid recompiling the model and less something in the model code changes.

Let's sample from the model now:

```{r}
fit <- stan("stan-models/lm.stan", chains = 4, iter = 2000,
  data = list(x = dat$x, y = dat$y, N = length(dat$y)))
```

```{r}
fit
```

```{r, eval=FALSE}
shinystan::launch_shinystan(fit)
```

```{r}
plot(fit)
```


Experiment with inspecting the posterior chains using the bayesplot package.

```{r}
fit_array <- as.array(fit)
pars <- c("alpha", "beta", "sigma")
bayesplot::mcmc_trace(fit_array, pars = pars)
bayesplot::mcmc_dens_overlay(fit_array, pars = pars)
```

Experiment with inspecting the posterior predictive distribution using the bayesplot package:

```{r}
pp <- extract(fit)$posterior_predictions
bayesplot::ppc_dens_overlay(y = dat$y, yrep = pp[1:25, ])
```

Congratulations --- you fit your first handwritten Stan model! We can do everything with the posterior samples that we could do with the samples from an rstanarm model or a brms model.

Some of the built-in helper functions and those packages won't work with our model though.

Let's say we want to extract linear model predictions from the posterior. We can do that with the existing samples in R or we could program that into the `generated quantities` section of our Stan model.

Let's start by doing it in R:

```{r}
post <- extract(fit)
```

The output from `rstan::extract()` is a named list. Each element of the list is a numeric vector of samples if that parameter had one dimension (as all of our parameters did this time), or a matrix of samples if, say, beta had represented multiple slope parameters. 

```{r}
names(post)
dim(post$beta)
```

There are a variety of more slick ways to do this, but I'm going to do it in the most simple explicit ways so you can see what's happening.

<https://github.com/mjskay/tidybayes>

```{r}
N <- 100
ggplot(dat, aes(x, y)) + geom_point() +
  geom_abline(
    intercept = post$alpha[1:N], 
    slope = post$beta[1:N], 
    alpha = 0.2)
```

Is that starting to look like the confidence/credible intervals are used to looking at?

How would we derive posterior predictions? In other words, how would we add on the observation component to our predictions? Let's manually create 8 draws from the posterior predictive distribution and compare them to our data. 

```{r}
n_sim <- 8
out <- vector(mode = "list", length = 8)
for (i in seq_along(out)) {
  out[[i]] <- tibble(x = x)
  out[[i]]$y <- 
    rnorm(length(x), 
      mean = post$alpha[i] + post$beta[i] * x, 
      sd = post$sigma[i])
  out[[i]]$i <- i
}
out <- bind_rows(out)
out$type <- "sampled"
out <- bind_rows(out, tibble(x = x, y = y, type = "observed", i = 9))

ggplot(out, aes(x, y, colour = type)) + geom_point() + 
  facet_wrap(~i)
```

